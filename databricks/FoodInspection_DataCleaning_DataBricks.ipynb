{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd33de50-34dc-41a8-b9b1-f326b8422bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, concat ,rtrim ,upper, trim , current_date\n",
    "from pyspark.sql.types import IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import split, regexp_replace, col\n",
    "from pyspark.sql.functions import split, regexp_replace, col\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d21e299-022e-4b4a-b9af-ede1c2151f87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widgets"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('sfURL', '')\n",
    "dbutils.widgets.text('sfUser', '')\n",
    "dbutils.widgets.text('sfDatabase', '')\n",
    "dbutils.widgets.text('sourceSchema', '')\n",
    "dbutils.widgets.text('sfWarehouse', '')\n",
    "dbutils.widgets.text('sfRole', '')\n",
    "dbutils.widgets.text('targetSchema','')\n",
    "dbutils.widgets.text('srcTable','')\n",
    "dbutils.widgets.text('targetTable','')\n",
    "\n",
    "\n",
    "sfURL = dbutils.widgets.get('sfURL')\n",
    "sfUser = dbutils.widgets.get('sfUser')\n",
    "sfPassword = dbutils.secrets.get(scope=\"snowflake-secrets\", key=\"sf-password\")\n",
    "sfDatabase = dbutils.widgets.get('sfDatabase')\n",
    "sourceSchema = dbutils.widgets.get('sourceSchema')\n",
    "srcTable = dbutils.widgets.get('srcTable')\n",
    "sfWarehouse = dbutils.widgets.get('sfWarehouse')\n",
    "sfRole = dbutils.widgets.get('sfRole')\n",
    "targetSchema = dbutils.widgets.get('targetSchema')\n",
    "targetTable =  dbutils.widgets.get('targetTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703e4084-f541-4114-a8b9-e18a3e1d50ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sfOptions = {\n",
    "    \"sfURL\": sfURL,\n",
    "    \"sfUser\": sfUser,\n",
    "    \"sfPassword\": sfPassword,\n",
    "    \"sfDatabase\": sfDatabase,\n",
    "    \"sfSchema\": sourceSchema,\n",
    "    \"sfWarehouse\": sfWarehouse,\n",
    "    \"sfRole\": sfRole\n",
    "}\n",
    "\n",
    "sfOptionsTarget = {\n",
    "    \"sfURL\": sfURL,\n",
    "    \"sfUser\": sfUser,\n",
    "    \"sfPassword\": sfPassword,\n",
    "    \"sfDatabase\": sfDatabase,\n",
    "    \"sfSchema\": targetSchema,\n",
    "    \"sfWarehouse\": sfWarehouse,\n",
    "    \"sfRole\": sfRole\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db803b2-e7f9-4ca4-a7b3-f76693210cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records: 18931\n+-------------------------------+------------+-------------+\n|LAT_LONG_LOCATION              |Lat_split   |Long_split   |\n+-------------------------------+------------+-------------+\n|\\r                             |NULL        |NULL         |\n|(37.774192712, -89.359825991)\\r|37.774192712|-89.359825991|\n|\\r                             |NULL        |NULL         |\n|\\r                             |NULL        |NULL         |\n|\\r                             |NULL        |NULL         |\n|\\r                             |NULL        |NULL         |\n|(32.763332, -96.855978)\\r      |32.763332   |-96.855978   |\n|\\r                             |NULL        |NULL         |\n|\\r                             |NULL        |NULL         |\n|(32.93083, -96.82094)\\r        |32.93083    |-96.82094    |\n+-------------------------------+------------+-------------+\nonly showing top 10 rows\n\nDallas data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if srcTable == 'CHICAGO_FOOD_INSPECTIONS_RAW':\n",
    "        try:\n",
    "            #Read Chicago Raw\n",
    "            df_raw = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", srcTable).load()\n",
    "            # Drop duplicates from all columns\n",
    "            df_final = df_raw.dropDuplicates()\n",
    "\n",
    "            # Display the final row count\n",
    "            row_count = df_final.count()\n",
    "            print(f\"Total rows after dropping duplicates: {row_count}\")\n",
    "            df_typed = df_final \\\n",
    "                        .withColumn(\"Inspection_ID\", col(\"Inspection_ID\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"License_Number\", col(\"License_Number\").cast(DoubleType())) \\\n",
    "                        .withColumn(\"Zip\", col(\"Zip\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"Latitude\", col(\"Latitude\").cast(DoubleType())) \\\n",
    "                        .withColumn(\"Longitude\", col(\"Longitude\").cast(DoubleType())) \\\n",
    "                        .withColumn(\"Location\", col(\"Location\").cast(DoubleType())) \\\n",
    "                        .withColumn(\"Inspection_Date\", col(\"Inspection_Date\").cast(TimestampType()))\n",
    "\n",
    "            df_cleaned = df_typed.withColumn(\"Risk\", when(col(\"Risk\").isNull(), \"Unknown\").otherwise(col(\"Risk\"))) \\\n",
    "               .withColumn(\"Violations\", when(col(\"Violations\").isNull(), \"Unknown\").otherwise(col(\"Violations\"))) \\\n",
    "               .withColumn(\"AKA_Name\", when(col(\"AKA_Name\").isNull(), \"Unknown\").otherwise(col(\"AKA_Name\"))) \\\n",
    "               .withColumn(\"Facility_Type\", when(col(\"Facility_Type\").isNull(), \"Unknown\").otherwise(col(\"Facility_Type\"))) \\\n",
    "               .withColumn(\"License_Number\", when((col(\"License_Number\").isNull()) | (col(\"License_Number\") == 0), -1).otherwise(col(\"License_Number\"))) \\\n",
    "               .withColumn(\"Zip\", when(col(\"Zip\").isNull(), -1).otherwise(col(\"Zip\")))\n",
    "            # Step 1: Get the mean of Latitude and Longitude columns\n",
    "            mean_lat_row = df_cleaned.selectExpr(\"avg(Latitude)\").first()\n",
    "            mean_long_row = df_cleaned.selectExpr(\"avg(Longitude)\").first()\n",
    "\n",
    "            # Step 2: Extract float values safely\n",
    "            mean_lat = float(mean_lat_row[0]) if mean_lat_row and mean_lat_row[0] is not None else 0.0\n",
    "            mean_long = float(mean_long_row[0]) if mean_long_row and mean_long_row[0] is not None else 0.0\n",
    "\n",
    "            # Step 3: Replace NULLs in Latitude, Longitude, and Location\n",
    "            df_cleaned = df_cleaned.withColumn(\n",
    "                \"Latitude\",\n",
    "                when(col(\"Latitude\").isNull(), lit(mean_lat)).otherwise(col(\"Latitude\"))\n",
    "            ).withColumn(\n",
    "                \"Longitude\",\n",
    "                when(col(\"Longitude\").isNull(), lit(mean_long)).otherwise(col(\"Longitude\"))\n",
    "            ).withColumn(\n",
    "                \"Location\",\n",
    "                when(col(\"Location\").isNull(),\n",
    "                    concat(lit(\"(\"), lit(mean_lat), lit(\", \"), lit(mean_long), lit(\")\"))\n",
    "                ).otherwise(col(\"Location\"))\n",
    "            )\n",
    "\n",
    "            # Step 4: Count nulls for validation\n",
    "            null_lat_count = df_cleaned.filter(col(\"Latitude\").isNull()).count()\n",
    "            null_long_count = df_cleaned.filter(col(\"Longitude\").isNull()).count()\n",
    "            null_location_count = df_cleaned.filter(col(\"Location\").isNull()).count()\n",
    "\n",
    "            print(f\"NULL Latitude count: {null_lat_count}\")\n",
    "            print(f\"NULL Longitude count: {null_long_count}\")\n",
    "            print(f\"NULL Location count: {null_location_count}\")\n",
    "\n",
    "            # Filter rows where Violations ends with whitespace\n",
    "            trailing_ws_count = df_cleaned.filter(col(\"Violations\").rlike(r\"\\s+$\")).count()\n",
    "\n",
    "            print(f\"Rows with trailing whitespaces in 'Violations': {trailing_ws_count}\")\n",
    "\n",
    "            df_final_cleaned = df_cleaned.withColumn(\"Violations\", rtrim(col(\"Violations\")))\n",
    "            # Step 1: Normalize City (uppercase + trimmed)\n",
    "            df_cleaned = df_final_cleaned.withColumn(\"City\", upper(trim(col(\"City\")))) \\\n",
    "                        .withColumn(\"State\", upper(trim(col(\"State\"))))\n",
    "\n",
    "\n",
    "            # Step 2: Fix Chicago-like values (replace all known variants with \"CHICAGO\")\n",
    "            df_cleaned = df_cleaned.withColumn(\n",
    "                \"City\",\n",
    "                when(col(\"City\").rlike(\"^(CHICAGO|CCHICAGO|CHICAGOO|CHICAGO\\\\.|CHICAGOC|CHICAGOCHICAGO|CH)$\"), \"CHICAGO\")\n",
    "                .otherwise(col(\"City\"))\n",
    "            )\n",
    "\n",
    "            # Step 3: Keep only rows where City == \"CHICAGO\" and State == \"IL\"\n",
    "            df_final = df_cleaned.filter((col(\"City\") == \"CHICAGO\") & (col(\"State\") == \"IL\"))\\\n",
    "                .withColumn(\"DI_JOB_ID\", lit(\"Amruta_01\")).withColumn(\"DI_LOAD_DT\", current_date())\n",
    "\n",
    "            df_final.select(\"City\", \"State\").distinct().show()\n",
    "            df_final.count()\n",
    "            df_final.write.format(\"snowflake\").options(**sfOptionsTarget).option(\"dbtable\", targetTable).mode(\"overwrite\").save()\n",
    "            print(\"Chicago data cleaning completed.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Chicago cleaning failed: {str(e)}\")\n",
    "\n",
    "    elif srcTable == 'DALLAS_FOOD_INSPECTIONS_RAW':\n",
    "        try:\n",
    "            df_raw = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", srcTable).load()\n",
    "            # Count duplicates based on all columns (like GROUP BY + HAVING > 1)\n",
    "            dup_count = df_raw.groupBy(df_raw.columns) \\\n",
    "                .count() \\\n",
    "                .filter(\"count > 1\") \\\n",
    "                .count()\n",
    "            print(f\"Number of duplicate records: {dup_count}\")\n",
    "            df_deduped = df_raw.dropDuplicates()\n",
    "            # Step 1: Remove parentheses from the LAT_LONG_LOCATION column\n",
    "            df_cleaned = df_deduped.withColumn(\"latlong_cleaned\", regexp_replace(\"LAT_LONG_LOCATION\", \"[()]\", \"\"))\n",
    "\n",
    "            # Step 2: Split on comma\n",
    "            df_cleaned = df_cleaned.withColumn(\"latlong_split\", split(col(\"latlong_cleaned\"), \",\"))\n",
    "\n",
    "            # Step 3: Create new columns for latitude and longitude as floats\n",
    "            df_cleaned = df_cleaned.withColumn(\"Lat_split\", col(\"latlong_split\").getItem(0).cast(\"double\")) \\\n",
    "                                .withColumn(\"Long_split\", col(\"latlong_split\").getItem(1).cast(\"double\"))\n",
    "            df_cleaned = df_cleaned.drop(\"latlong_cleaned\", \"latlong_split\")\n",
    "            df_cleaned.select(\"LAT_LONG_LOCATION\", \"Lat_split\", \"Long_split\").show(10, truncate=False)\n",
    "            df_cleaned = df_cleaned.withColumn(\"ZIP_CODE\", split(col(\"ZIP_CODE\"), \"-\").getItem(0))\n",
    "            df_cleaned = df_cleaned.withColumn(\"ZIP_CODE\", col(\"ZIP_CODE\").cast(\"long\"))\n",
    "            df_cleaned = df_cleaned.filter((col(\"zip_code\")>= 75201) & (col(\"zip_code\")<= 75398))\n",
    "            #median for Lat\n",
    "            lat_values = df_cleaned.filter(col(\"Lat_split\").isNotNull()) \\\n",
    "                                .select(\"Lat_split\") \\\n",
    "                                .rdd.map(lambda row: row[0]) \\\n",
    "                                .sortBy(lambda x: x) \\\n",
    "                                .collect()\n",
    "\n",
    "            lat_median = lat_values[len(lat_values) // 2]\n",
    "\n",
    "            #median for Long\n",
    "            long_values = df_cleaned.filter(col(\"Long_split\").isNotNull()) \\\n",
    "                                    .select(\"Long_split\") \\\n",
    "                                    .rdd.map(lambda row: row[0]) \\\n",
    "                                    .sortBy(lambda x: x) \\\n",
    "                                    .collect()\n",
    "\n",
    "            long_median = long_values[len(long_values) // 2]\n",
    "\n",
    "\n",
    "            #replacing nulls with median\n",
    "            df_cleaned = df_cleaned.fillna({\n",
    "                \"Lat_split\": lat_median,\n",
    "                \"Long_split\": long_median\n",
    "            })\n",
    "            df_fixed = df_cleaned.withColumn(\"Lat_split\",when((col(\"Lat_split\") < 32.6) | (col(\"Lat_split\") > 33.0), lit(lat_median))\n",
    "                                             .otherwise(col(\"Lat_split\"))).withColumn(\"Long_split\", when((col(\"Long_split\") < -97.0) | (col(\"Long_split\") > -96.6), lit(long_median)).otherwise(col(\"Long_split\")))\n",
    "            cols_to_replace = {\n",
    "                            \"STREET_DIRECTION\": \"N/A - Not Available\",\n",
    "                            \"STREET_TYPE\": \"N/A - Not Available\",\n",
    "                            \"STREET_UNIT\": \"N/A - Not Available\"\n",
    "                            }\n",
    "\n",
    "            df_fixed = df_fixed.fillna(cols_to_replace)\n",
    "            # Step 1: Separate columns by data type\n",
    "            violation_string_cols = []\n",
    "            violation_points_cols = []\n",
    "\n",
    "            for i in range(1, 26):\n",
    "                violation_string_cols.extend([\n",
    "                    f\"VIOLATION_DESCRIPTION_{i}\",\n",
    "                    f\"VIOLATION_DETAILS_{i}\",\n",
    "                    f\"VIOLATION_MEMO_{i}\"\n",
    "                ])\n",
    "                violation_points_cols.append(f\"VIOLATION_POINTS_{i}\")\n",
    "\n",
    "            # Step 2: Replace nulls in string columns with \"N/A\"\n",
    "            replace_string_dict = {col: \"N/A - Not Applicable\" for col in violation_string_cols}\n",
    "            df_fixed = df_fixed.fillna(replace_string_dict)\n",
    "\n",
    "            # Step 3: Replace nulls in numeric (points) columns with 0\n",
    "            replace_numeric_dict = {col: 0 for col in violation_points_cols}\n",
    "            df_fixed = df_fixed.fillna(replace_numeric_dict)\n",
    "                        # 1. Drop the original LAT_LONG_LOCATION column\n",
    "            df_fixed = df_fixed.drop(\"LAT_LONG_LOCATION\")\n",
    "\n",
    "            # 2. Cast ZIP_CODE to Integer\n",
    "            df_fixed = df_fixed.withColumn(\"ZIP_CODE\", col(\"ZIP_CODE\").cast(IntegerType()))\n",
    "\n",
    "            # 3. Cast INSPECTION_SCORE and STREET_NUMBER to Integer\n",
    "            df_fixed = df_fixed \\\n",
    "                .withColumn(\"INSPECTION_SCORE\", col(\"INSPECTION_SCORE\").cast(IntegerType())) \\\n",
    "                .withColumn(\"STREET_NUMBER\", col(\"STREET_NUMBER\").cast(IntegerType()))\n",
    "\n",
    "            # 4. Rename and cast Lat/Long to Double (for Snowflake FLOAT compatibility)\n",
    "            df_fixed = df_fixed \\\n",
    "                .withColumnRenamed(\"Lat_split\", \"LATITUDE\") \\\n",
    "                .withColumnRenamed(\"Long_split\", \"LONGITUDE\") \\\n",
    "                .withColumn(\"LATITUDE\", col(\"LATITUDE\").cast(DoubleType())) \\\n",
    "                .withColumn(\"LONGITUDE\", col(\"LONGITUDE\").cast(DoubleType()))\n",
    "\n",
    "            # 5. Cast all VIOLATION_POINTS_* columns to Integer\n",
    "            for i in range(1, 26):\n",
    "                df_fixed = df_fixed.withColumn(f\"VIOLATION_POINTS_{i}\", col(f\"VIOLATION_POINTS_{i}\").cast(IntegerType()))\n",
    "\n",
    "            # 6. Add audit columns\n",
    "            df_fixed = df_fixed \\\n",
    "                .withColumn(\"DI_JOB_ID\", lit(\"SP_001\")) \\\n",
    "                .withColumn(\"DI_LOAD_DT\", current_date())\n",
    "\n",
    "            # 7. Reorder columns to move audit fields to the end\n",
    "            cols_without_audit = [c for c in df_fixed.columns if c not in [\"DI_JOB_ID\", \"DI_LOAD_DT\"]]\n",
    "            final_column_order = cols_without_audit + [\"DI_JOB_ID\", \"DI_LOAD_DT\"]\n",
    "            df_final = df_fixed.select(*final_column_order)\n",
    "            df_final.write.format(\"snowflake\").options(**sfOptionsTarget).option(\"dbtable\", targetTable).mode(\"overwrite\").save()\n",
    "\n",
    "            print(\"Dallas data cleaning completed.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Dallas cleaning failed: {str(e)}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported src_table: '{src_table}'. Only 'DALLAS_FOOD_INSPECTIONS_RAW' or 'CHICAGO_FOOD_INSPECTIONS_RAW' are supported.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"🔥 Error during cleaning: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff812615-6faf-49ae-b4fc-5b19254460e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FoodInspection_Data_Cleaning",
   "widgets": {
    "sfDatabase": {
     "currentValue": "FOOD_INSPECTION_DB",
     "nuid": "15848ba1-5232-41d0-b07a-a04c43d77017",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sfDatabase",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "sfDatabase",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sfRole": {
     "currentValue": "DEV_ROLE",
     "nuid": "49bab2c4-756e-4360-a94b-9934488c571b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sfRole",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "sfRole",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sfURL": {
     "currentValue": "AQDGCDE-JLA48380.snowflakecomputing.com",
     "nuid": "d78f2c5e-56d1-45d5-8bd5-30e2f16ae4fb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sfURL",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "sfURL",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sfUser": {
     "currentValue": "ADF_USER",
     "nuid": "41d521e4-1670-4c9a-85f7-0bbd42f8591b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sfUser",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "sfUser",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sfWarehouse": {
     "currentValue": "DADABI_WH",
     "nuid": "59c4de7b-fad4-4d48-8156-8522599921e6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sfWarehouse",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "sfWarehouse",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sourceSchema": {
     "currentValue": "BRONZE",
     "nuid": "b48d1011-174b-4ace-a914-54bb5fb304e3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sourceSchema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "sourceSchema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "srcTable": {
     "currentValue": "DALLAS_FOOD_INSPECTIONS_RAW",
     "nuid": "8400e426-ea8b-46f7-acd0-c602e3e1bb88",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "srcTable",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "srcTable",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "targetSchema": {
     "currentValue": "SILVER",
     "nuid": "3d2cd61c-ef7a-4ace-a10f-2835895a1f77",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "targetSchema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "targetSchema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "targetTable": {
     "currentValue": "DALLAS_FOOD_INSPECTIONS_STG1",
     "nuid": "44d95581-3d92-47e2-acac-acc3bc628812",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "targetTable",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "targetTable",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}